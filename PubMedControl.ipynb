{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PubMedControl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOs+eQY3ilAocCorLCqjnpY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YujiSue/python/blob/master/PubMedControl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtT4GOqpLqUG"
      },
      "source": [
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIeJ5MckK6YR"
      },
      "source": [
        "import requests\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import threading\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "driver.implicitly_wait(10)\n",
        "headers = requests.utils.default_headers()\n",
        "headers[\"User-Agent\"] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:76.0) Gecko/20100101 Firefox/76.0'\n",
        "\n",
        "class PubMedSearcher:\n",
        "  def __init__(self):\n",
        "    self.que = ''\n",
        "    self.param = { 'db': 'pubmed', 'retmode': 'json' }\n",
        "    self.reflist = {}\n",
        "    self.base = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?'\n",
        "\n",
        "  def setParam(self, key, value):\n",
        "    self.param[key] = value\n",
        "\n",
        "  def makeQue(self):\n",
        "    self.que = self.base\n",
        "    if (0 < len(self.param)):\n",
        "      for key in self.param:\n",
        "        self.que += key + \"=\" + self.param[key] + '&'\n",
        "      self.que = self.que[:len(self.que)-1]    \n",
        "  \n",
        "  def getSummary(self, id):\n",
        "    response = requests.get('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&retmode=json&id='+str(id))\n",
        "    summary = response.json()\n",
        "    if (summary.get('result') == None):\n",
        "      print(summary)\n",
        "    else: \n",
        "      self.reflist[str(id)] = summary['result'][str(id)]\n",
        "\n",
        "  def search(self):\n",
        "    self.makeQue()\n",
        "    response  = requests.get(self.que+'&rettype=count')\n",
        "    total = response.json()['esearchresult']['count']\n",
        "    print('count:',total)\n",
        "    if (0 < int(total)):\n",
        "      response = requests.get(self.que+'&retmax='+str(total))\n",
        "      ids = response.json()['esearchresult']['idlist']\n",
        "      for val in ids:\n",
        "        self.getSummary(val)\n",
        "        time.sleep(0.25)\n",
        "\n",
        "  def clear(self):\n",
        "    self.reflist = {}\n",
        "\n",
        "  def load(self, path):\n",
        "    self.reflist = json.load(open(path, 'r'))\n",
        "\n",
        "  def output(self, path):\n",
        "    with open(path, 'w') as f:\n",
        "      json.dump(self.reflist, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "def elsevierLink(url, doi):\n",
        "  response = requests.get('https://api.elsevier.com/content/object/doi/'+doi+'?httpAccept=application/json')\n",
        "  return response.json()['attachment-metadata-response']['coredata']['link'][1]['@href']\n",
        "\n",
        "def checkRedirect(url):\n",
        "  response = requests.get(url, headers=headers)\n",
        "  if (response.headers.get('Link') != None):\n",
        "    beg = response.headers['Link'].find('<')\n",
        "    if (beg != -1):\n",
        "      end = response.headers['Link'].find('>', beg+1)\n",
        "      return response.headers['Link'][beg+1:end]\n",
        "    else:\n",
        "      return response.headers['Link']\n",
        "  else:\n",
        "    return url\n",
        "\n",
        "class JournalSearch:\n",
        "  def __init__(self):\n",
        "    self.base = 'https://pubmed.ncbi.nlm.nih.gov/'\n",
        "\n",
        "  def search(self, id, key, eid):\n",
        "    words = []\n",
        "    fulltext = {}\n",
        "    trial = 0\n",
        "    while (True):\n",
        "      try:\n",
        "        response = requests.get(self.base+id, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        links = soup.select('div.full-text-links-list a')\n",
        "        if (0 < len(links)):\n",
        "          for item in links:\n",
        "            if (item['href'].find('elsevier') != -1):\n",
        "              fulltext[item['title']] = elsevierLink(item['href'], eid)\n",
        "            else:\n",
        "              fulltext[item['title']] = checkRedirect(item['href'])\n",
        "            print(id,':',fulltext[item['title']])\n",
        "        break\n",
        "      except Exception as e:\n",
        "        print('ID:', id, '\\t', e)\n",
        "        trial = trial + 1\n",
        "        if (trial == 3):\n",
        "          break\n",
        "    trial = 0\n",
        "    if (0 < len(fulltext)):\n",
        "      for site in fulltext:\n",
        "        while (True):\n",
        "          try:\n",
        "            response = requests.get(fulltext[site], headers=headers)\n",
        "            words = re.findall(key, response.text)\n",
        "            print(id,':',words)\n",
        "            break\n",
        "          except Exception as e:\n",
        "            trial = trial + 1\n",
        "            if (trial == 3):\n",
        "              print('ID:', id, '\\t', e)\n",
        "              break\n",
        "        if (0 < len(words)):\n",
        "          break\n",
        "    return words\n",
        "\n",
        "def worker(tid, count, reflist, header, prefix):\n",
        "  beg = tid*count\n",
        "  end = beg+count\n",
        "  current = 0\n",
        "  result = '\"ID\",\"Link\",\"Title\",\"Author\",\"Journal\",\"PubDate\",\"Type\",\"KeyWords\"\\n' if header==True else ''\n",
        "  jsearcher = JournalSearch()\n",
        "  for refid in reflist:\n",
        "    if (current < beg):\n",
        "      current = current+1\n",
        "      continue\n",
        "    if (end <= current):\n",
        "      break\n",
        "    refdata = reflist[refid]\n",
        "    result += '\"'+refid+'\",\"'+'https://pubmed.ncbi.nlm.nih.gov/'+refid\n",
        "    title = refdata['title']\n",
        "    title = title.replace('&lt;i&gt;', '')\n",
        "    title = title.replace('&lt;/i&gt;', '')\n",
        "    title = title.replace('&lt;sub&gt;', '')\n",
        "    title = title.replace('&lt;/sub&gt;', '')\n",
        "    title = title.replace('&lt;sup&gt;', '')\n",
        "    title = title.replace('&lt;/sup&gt;', '')\n",
        "    result += '\",\"'+title\n",
        "    if (refdata.get('authors') != None and 0 < len(refdata['authors'])):\n",
        "      result += '\",\"'\n",
        "      names = ''\n",
        "      for author in refdata['authors']:\n",
        "        names += author['name']+','\n",
        "      result += names[0:len(names)-1]\n",
        "    else: \n",
        "      result += '\",\"'\n",
        "    result += '\",\"'+refdata['fulljournalname']\n",
        "    result += '\",\"'+refdata['pubdate']\n",
        "    if (refdata.get('pubtype') != None and 0 < len(refdata['pubtype'])):\n",
        "      result += '\",\"'+\"-\".join(refdata['pubtype'])\n",
        "    else: \n",
        "      result += '\",\"'\n",
        "    refdata['myquery'] = jsearcher.search(refid, 'tm\\d{3,}', refdata['elocationid'])\n",
        "    if (0 < len(refdata['myquery'])):\n",
        "      result += '\",\"'+\",\".join(refdata['myquery'])+'\"\\n'\n",
        "    else:\n",
        "      result += '\",\"\"\\n'\n",
        "    current = current+1\n",
        "    print(current-beg,'/',count)\n",
        "  f = open(prefix+str(tid).rjust(2, '0')+'.csv', 'w')\n",
        "  f.write(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAjdbeAcMkXY"
      },
      "source": [
        "#PubMedから検索\n",
        "searcher = PubMedSearcher()\n",
        "searcher.setParam('api_key', '<Your ID>')\n",
        "#検索クエリ\n",
        "searcher.setParam('term', 'elegans')\n",
        "#時間範囲\n",
        "searcher.setParam('mindate', '2019/04/01')\n",
        "searcher.setParam('maxdate', '2020/03/31')\n",
        "#検索\n",
        "searcher.search()\n",
        "#出力\n",
        "searcher.output('elegans_2019_reflist.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in43pHzoKACM"
      },
      "source": [
        "#出力したファイルを読み込む\n",
        "#上セルから連続実行する場合は下２行コメントアウト\n",
        "searcher = PubMedSearcher()\n",
        "searcher.load('elegans_2019_reflist.json')\n",
        "\n",
        "prefix = 'result2019-'\n",
        "#マルチスレッド化\n",
        "total = len(searcher.reflist)\n",
        "count = 20\n",
        "threads = []\n",
        "maxthreads = int(total/count)+1\n",
        "#for tid in range(maxthreads):\n",
        "for tid in range(3,4):\n",
        "  threads.append(threading.Thread(target=worker, args=(tid, count, searcher.reflist, False if 0 < tid else True, prefix)))\n",
        "for thread in threads:\n",
        "  thread.start()\n",
        "for thread in threads:\n",
        "  thread.join()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKlQCQBoR9qD"
      },
      "source": [
        "#統合\n",
        "integrated = ''\n",
        "for tid in range(maxthreads):\n",
        "  fr = open(prefix+str(tid).rjust(2, '0')+'.csv')\n",
        "  res = fr.read()\n",
        "  integrated += res\n",
        "fw = open(prefix[:len(prefix)-1]+'.csv', 'w')\n",
        "fw.write(integrated)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}